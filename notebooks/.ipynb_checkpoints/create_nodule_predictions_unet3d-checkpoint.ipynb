{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/watts/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ['THEANO_FLAGS'] = 'floatX=float32,device=gpu0,nvcc.fastmath=True,lib.cnmem=0.85'\n",
    "#THEANO_FLAGS=device=gpu python -c \"import theano; print(theano.sandbox.cuda.device_properties(0))\"\n",
    "import keras\n",
    "#import theano\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_curve, f1_score, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "from sklearn import metrics\n",
    "from skimage.transform import resize\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle as pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback\n",
    "try:\n",
    "    from tqdm import tqdm # long waits are not fun\n",
    "except:\n",
    "    print('TQDM does make much nicer wait bars...')\n",
    "    tqdm = lambda x: x\n",
    "\n",
    "#import matplotlib\n",
    "#matplotlib.use(\"Pdf\")\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "#from helper_functions import *\n",
    "\n",
    "from keras.layers import merge, Convolution2D, MaxPooling2D, Input, UpSampling2D\n",
    "from keras.layers import merge, Convolution3D, MaxPooling3D, Input, UpSampling3D\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_dim_ordering('th')  # Theano dimension ordering in this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "working_path = \"/home/watts/lal/Kaggle/lung_cancer/\"\n",
    "model_path = \"/home/watts/lal/Kaggle/lung_cancer/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_if_image_exists(fname):\n",
    "    fname = os.path.join(working_path+'data/stage1/stage1/', fname)\n",
    "    return os.path.exists(fname)\n",
    "\n",
    "def check_if_scan_exists(folder):\n",
    "    folder = os.path.join(working_path+'data/stage1/stage1/', folder)\n",
    "    return os.path.isdir(folder)\n",
    "\n",
    "def get_current_date():\n",
    "    return strftime('%Y%m%d')\n",
    "\n",
    "\n",
    "def load_images(df):\n",
    "    for i, row in df['ImageFile'].iterrows():\n",
    "        img = imread(row)\n",
    "        yield img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_np(y_true,y_pred):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 does not exists\n",
      "Empty DataFrame\n",
      "Columns: [id, cancer, scan_folder, exist]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(working_path+'data/stage1/stage1_labels.csv')\n",
    "\n",
    "df['scan_folder'] = df['id']\n",
    "\n",
    "df['exist'] = df['scan_folder'].apply(check_if_scan_exists)\n",
    "\n",
    "print '%i does not exists' % (len(df) - df['exist'].sum())\n",
    "print df[~df['exist']]\n",
    "\n",
    "df = df[df['exist']]\n",
    "df = df.reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inputs = Input((1, 16, 128, 128))\n",
    "\n",
    "# # output W2 = 128 x 128 x 16 x 32\n",
    "# conv1 = Convolution3D(32, 3, 3, 3, activation='relu', border_mode='same')(inputs)\n",
    "\n",
    "# # output 128 x 128 x 16 x 32\n",
    "# conv1 = Convolution3D(32, 3, 3, 3, activation='relu', border_mode='same')(conv1)\n",
    "\n",
    "# # output 64 x 64 x 8 x 32\n",
    "# pool1 = MaxPooling3D(pool_size=(2, 2, 2))(conv1)\n",
    "\n",
    "# # output 64 x 64 x 8 x 64\n",
    "# conv2 = Convolution3D(64, 3, 3, 3, activation='relu', border_mode='same')(pool1)\n",
    "\n",
    "# # output 64 x 64 x 8 x 64\n",
    "# conv2 = Convolution3D(64, 3, 3, 3, activation='relu', border_mode='same')(conv2)\n",
    "\n",
    "# # output 32 x 32 x 4 x 64\n",
    "# pool2 = MaxPooling3D(pool_size=(2, 2, 2))(conv2)\n",
    "\n",
    "# # output 32 x 32 x 4 x 128\n",
    "# conv3 = Convolution3D(128, 3, 3, 3, activation='relu', border_mode='same')(pool2)\n",
    "\n",
    "# #output 32 x 32 x 4 x 128\n",
    "# conv3 = Convolution3D(128, 3, 3, 3, activation='relu', border_mode='same')(conv3)\n",
    "\n",
    "# # output 16 x 16 x 2 x 128\n",
    "# pool3 = MaxPooling3D(pool_size=(2, 2, 2))(conv3)\n",
    "\n",
    "# # output 32 x 32 x 2 x 256\n",
    "# #conv4 = Convolution3D(256, 3, 3, 3, activation='relu', border_mode='same')(pool3)\n",
    "\n",
    "# #output 32 x 32 x 2 x 256\n",
    "# #conv4 = Convolution3D(256, 3, 3, 3, activation='relu', border_mode='same')(conv4)\n",
    "\n",
    "# #output 16 x 16 x 1 x 256\n",
    "# #pool4 = MaxPooling3D(pool_size=(2, 2, 2))(conv4)\n",
    "\n",
    "# #output 16 x 16 x 1 x 512\n",
    "# #conv5 = Convolution3D(512, 3, 3, 3, activation='relu', border_mode='same')(pool4)\n",
    "\n",
    "# #output 16 x 16 x 1 x 512\n",
    "# #conv5 = Convolution3D(512, 3, 3, 3, activation='relu', border_mode='same')(conv5)\n",
    "\n",
    "# #output of UpSampling2D(size=(2, 2, 2))(conv5): 32 x 32 x 2 x 512\n",
    "# #outputof up6: 32 x 32 x 2 x 768\n",
    "# #up6 = merge([UpSampling3D(size=(2, 2, 2))(conv5), conv4], mode='concat', concat_axis=1)\n",
    "\n",
    "# # output 16 x 16 x 2 x 256\n",
    "# #conv6 = Convolution3D(256, 3, 3, 3, activation='relu', border_mode='same')(up6)\n",
    "# conv6 = Convolution3D(256, 3, 3, 3, activation='relu', border_mode='same')(pool3)\n",
    "\n",
    "# # output 16 x 16 x 2 x 256\n",
    "# conv6 = Convolution3D(256, 3, 3, 3, activation='relu', border_mode='same')(conv6)\n",
    "\n",
    "# # output of UpSampling2D(size=(2, 2, 2))(conv6): 32 x 32 x 4x 256\n",
    "# # outputof up7: 32 x 32 x 4 x 384\n",
    "# up7 = merge([UpSampling3D(size=(2, 2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n",
    "\n",
    "# # output 32 x 32 x 4 x 128\n",
    "# conv7 = Convolution3D(128, 3, 3, 3, activation='relu', border_mode='same')(up7)\n",
    "\n",
    "# # output 32 x 32 x 4 x 128\n",
    "# conv7 = Convolution3D(128, 3, 3, 3, activation='relu', border_mode='same')(conv7)\n",
    "\n",
    "# # output of UpSampling2D(size=(2, 2, 2))(conv7): 64 x 64 x 8 x 128\n",
    "# # outputof up8: 64  x 64  x 8 x 192\n",
    "# up8 = merge([UpSampling3D(size=(2, 2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n",
    "\n",
    "# # output 64 x 64 x 8 x 64\n",
    "# conv8 = Convolution3D(64, 3, 3, 3, activation='relu', border_mode='same')(up8)\n",
    "\n",
    "# # output 64 x 64 x 8 x 64\n",
    "# conv8 = Convolution3D(64, 3, 3, 3, activation='relu', border_mode='same')(conv8)\n",
    "\n",
    "# # output of UpSampling2D(size=(2, 2, 2))(conv8): 128 x 128 x 16 x 64\n",
    "# # outputof up9: 128 x 128 x 16 x 96\n",
    "# up9 = merge([UpSampling3D(size=(2, 2, 2))(conv8), conv1], mode='concat', concat_axis=1)\n",
    "\n",
    "# # output 128 x 128 x 16 x 32\n",
    "# conv9 = Convolution3D(32, 3, 3, 3, activation='relu', border_mode='same')(up9)\n",
    "\n",
    "# # output 128 x 128 x 16 x 32\n",
    "# conv9 = Convolution3D(32, 3, 3, 3, activation='relu', border_mode='same')(conv9)\n",
    "\n",
    "# # output 128 x 128 x 16 x 1\n",
    "# conv10 = Convolution3D(1, 1, 1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "# model = Model(input=inputs, output=conv10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "\n",
    "# # returns a compiled model\n",
    "# model.load_weights(model_path+'unet3d_train.hdf5')\n",
    "# model.compile(optimizer=Adam(lr=1.0e-5), loss=dice_coef_loss, metrics=[dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_slices = 16\n",
    "img_width = 128\n",
    "img_height = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_arch = 'model_unet3d_%d.json' % img_width\n",
    "model_weights = 'model_unet3d_%d.h5' % img_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py:1202: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "json_file = open(model_path+model_arch, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "model.load_weights(model_path+model_weights)\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=1.0e-5), loss=dice_coef_loss, metrics=[dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 139/1397 [00:12<01:44, 12.09it/s]"
     ]
    }
   ],
   "source": [
    "IMG_PX_SIZE = img_width\n",
    "IMG_PX_SIZE_ORG = 512\n",
    "HM_SLICES = num_slices\n",
    "num_zeros = 0\n",
    "num_test = len(df)\n",
    "segmented_lung_slices = np.ndarray([num_test,1,num_slices,img_width,img_height],dtype=np.float32)\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    #if i != 0:\n",
    "    #    continue\n",
    "    fname = os.path.join('data/stage1/stage1/', row['scan_folder'])\n",
    "    try:\n",
    "        j = 0\n",
    "        scan_folder = row['scan_folder']\n",
    "\n",
    "        X_segmented_lung_fname = working_path+'cache/segmentation_1_1_1/X_segmented_lungs_%s_%s_%s_%s.npy' % (scan_folder, HM_SLICES, IMG_PX_SIZE, IMG_PX_SIZE)\n",
    "        X_nodule_fname = working_path+'cache/X_nodule_%s_%s_%s_%s.npy' % (scan_folder, HM_SLICES, IMG_PX_SIZE, IMG_PX_SIZE)\n",
    "        #print '0..'\n",
    "        segmented_lung = np.load(X_segmented_lung_fname)\n",
    "        #print segmented_lung.shape\n",
    "        #segmented_lung = resize(segmented_lung,[16, 128,128])\n",
    "        segmented_lung = segmented_lung[0]\n",
    "        #print segmented_lung.shape\n",
    "        \n",
    "        nz = np.count_nonzero(segmented_lung)\n",
    "        if nz == 0:\n",
    "            print 'slice is 0...'\n",
    "            num_zeros += 1\n",
    "        #print '1..'\n",
    "        segmented_lung_slices[i,0] = segmented_lung\n",
    "        my_nodules_mask = model.predict([segmented_lung_slices[i:i+1]], verbose=0)[0]\n",
    "        #print '2..'\n",
    "        np.save(X_nodule_fname, my_nodules_mask)\n",
    "    except:\n",
    "        print '%s has failed' % i\n",
    "        #sys.exit(1)\n",
    "print 'Done'\n",
    "now = datetime.datetime.now()\n",
    "print now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 does not exists\n",
      "Empty DataFrame\n",
      "Columns: [id, cancer, scan_folder, exist]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(working_path+'data/stage1/stage1_sample_submission.csv')\n",
    "\n",
    "df['scan_folder'] = df['id']\n",
    "\n",
    "df['exist'] = df['scan_folder'].apply(check_if_scan_exists)\n",
    "\n",
    "print '%i does not exists' % (len(df) - df['exist'].sum())\n",
    "print df[~df['exist']]\n",
    "\n",
    "df = df[df['exist']]\n",
    "df = df.reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [00:17<00:00, 11.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "2017-04-08 05:34:00.591795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "IMG_PX_SIZE = img_width\n",
    "IMG_PX_SIZE_ORG = 512\n",
    "HM_SLICES = num_slices\n",
    "num_zeros = 0\n",
    "num_test = len(df)\n",
    "#print num_test\n",
    "test_lung_slices = np.ndarray([num_test,1,num_slices,img_width,img_height],dtype=np.float32)\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "#     if i != 0:\n",
    "#        continue\n",
    "    fname = os.path.join('data/stage1/stage1/', row['scan_folder'])\n",
    "    #print fname\n",
    "    try:\n",
    "        j = 0\n",
    "        scan_folder = row['scan_folder']\n",
    "        #print scan_folder\n",
    "        X_test_segmented_lung_fname = working_path+'cache/segmentation_1_1_1/X_test_segmented_lungs_%s_%s_%s_%s.npy' % (scan_folder, HM_SLICES, IMG_PX_SIZE, IMG_PX_SIZE)\n",
    "        #print '0a..'\n",
    "        #print(X_test_segmented_lung_fname)\n",
    "        X_test_nodule_fname = working_path+'cache/X_test_nodule_%s_%s_%s_%s.npy' % (scan_folder, HM_SLICES, IMG_PX_SIZE, IMG_PX_SIZE)\n",
    "        #print '0..'\n",
    "        segmented_lung = np.load(X_test_segmented_lung_fname)\n",
    "        #print segmented_lung.shape\n",
    "        #segmented_lung = resize(segmented_lung,[16, 128,128])\n",
    "        segmented_lung = segmented_lung[0]\n",
    "        #print segmented_lung.shape\n",
    "        \n",
    "        nz = np.count_nonzero(segmented_lung)\n",
    "        if nz == 0:\n",
    "            print 'slice is 0...'\n",
    "            num_zeros += 1\n",
    "            continue\n",
    "        #print '1..'\n",
    "        test_lung_slices[i,0] = segmented_lung\n",
    "        my_nodules_mask = model.predict([test_lung_slices[i:i+1]], verbose=0)[0]\n",
    "        #print '2..'\n",
    "        np.save(X_test_nodule_fname, my_nodules_mask)\n",
    "    except:\n",
    "        print '%s has failed' % i\n",
    "        #sys.exit(1)\n",
    "print 'Done'\n",
    "now = datetime.datetime.now()\n",
    "print now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
