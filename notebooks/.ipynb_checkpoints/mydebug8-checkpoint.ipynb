{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne import layers\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "PY2 = sys.version_info[0] == 2\n",
    "\n",
    "if PY2:\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "    def pickle_load(f, encoding):\n",
    "        return pickle.load(f)\n",
    "else:\n",
    "    from urllib.request import urlretrieve\n",
    "\n",
    "    def pickle_load(f, encoding):\n",
    "        return pickle.load(f, encoding=encoding)\n",
    "\n",
    "DATA_URL = 'http://deeplearning.net/data/mnist/mnist.pkl.gz'\n",
    "DATA_FILENAME = 'mnist.pkl.gz'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def _load_data(url=DATA_URL, filename=DATA_FILENAME):\n",
    "    \"\"\"Load data from `url` and store the result in `filename`.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Downloading MNIST dataset\")\n",
    "        urlretrieve(url, filename)\n",
    "\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        return pickle_load(f, encoding='latin-1')\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Get data with labels, split into training, validation and test set.\"\"\"\n",
    "    data = _load_data()\n",
    "    print data.shape\n",
    "    X_train, y_train = data[0]\n",
    "    X_valid, y_valid = data[1]\n",
    "    X_test, y_test = data[2]\n",
    "    y_train = numpy.asarray(y_train, dtype=numpy.int32)\n",
    "    y_valid = numpy.asarray(y_valid, dtype=numpy.int32)\n",
    "    y_test = numpy.asarray(y_test, dtype=numpy.int32)\n",
    "\n",
    "    return dict(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_valid=X_valid,\n",
    "        y_valid=y_valid,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        num_examples_train=X_train.shape[0],\n",
    "        num_examples_valid=X_valid.shape[0],\n",
    "        num_examples_test=X_test.shape[0],\n",
    "        input_dim=X_train.shape[1],\n",
    "        output_dim=10,\n",
    "    )\n",
    "\n",
    "\n",
    "def nn_example(data):\n",
    "    net1 = NeuralNet(\n",
    "        layers=[('input', layers.InputLayer),\n",
    "                ('hidden', layers.DenseLayer),\n",
    "                ('output', layers.DenseLayer),\n",
    "                ],\n",
    "        # layer parameters:\n",
    "        input_shape=(None, 28*28),\n",
    "        hidden_num_units=100,  # number of units in 'hidden' layer\n",
    "        output_nonlinearity=lasagne.nonlinearities.softmax,\n",
    "        output_num_units=10,  # 10 target values for the digits 0, 1, 2, ..., 9\n",
    "\n",
    "        # optimization method:\n",
    "        update=nesterov_momentum,\n",
    "        update_learning_rate=0.01,\n",
    "        update_momentum=0.9,\n",
    "\n",
    "        max_epochs=10,\n",
    "        verbose=1,\n",
    "        )\n",
    "\n",
    "    # Train the network\n",
    "    net1.fit(data['X_train'], data['y_train'])\n",
    "\n",
    "    # Try the network on new data\n",
    "    print(\"Feature vector (100-110): %s\" % data['X_test'][0][100:110])\n",
    "    print(\"Label: %s\" % str(data['y_test'][0]))\n",
    "    print(\"Predicted: %s\" % str(net1.predict([data['X_test'][0]])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    data = load_data()\n",
    "    print(\"Got %i testing datasets.\" % len(data['X_train']))\n",
    "    nn_example(data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "The output is\n",
    "\n",
    "# Neural Network with 79510 learnable parameters\n",
    "\n",
    "## Layer information\n",
    "\n",
    "  #  name      size\n",
    "---  ------  ------\n",
    "  0  input      784\n",
    "  1  hidden     100\n",
    "  2  output      10\n",
    "\n",
    "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
    "-------  ------------  ------------  -----------  -----------  -----\n",
    "      1       0.59132       0.32314      1.82993      0.90988  1.70s\n",
    "      2       0.30733       0.26644      1.15348      0.92623  1.96s\n",
    "      3       0.25879       0.23606      1.09629      0.93363  2.09s\n",
    "      4       0.22680       0.21424      1.05865      0.93897  2.13s\n",
    "      5       0.20187       0.19633      1.02827      0.94313  2.21s\n",
    "      6       0.18129       0.18187      0.99685      0.94758  1.81s\n",
    "      7       0.16398       0.16992      0.96506      0.95074  2.14s\n",
    "      8       0.14941       0.16020      0.93265      0.95262  1.88s\n",
    "      9       0.13704       0.15189      0.90222      0.95460  2.15s\n",
    "     10       0.12633       0.14464      0.87342      0.95707  2.21s\n",
    "Feature vector (100-110): [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "Label: 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
